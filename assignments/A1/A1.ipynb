{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI4A-T0z5AU6"
   },
   "source": [
    "# Assignment 1\n",
    "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEomoMzH5Nf6"
   },
   "source": [
    "## 1 $n$-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsSAtTqt7Q8a"
   },
   "outputs": [],
   "source": [
    "!wget -O train.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/train.txt\n",
    "!wget -O dev.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/dev.txt\n",
    "!wget -O test.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ElrINWW7oF7"
   },
   "source": [
    "### 1.1 Building vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eawcuVV19kZm"
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q-rNT_QL8Dvt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 22630\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# So, build the model based on make the vocabulary set on training set\n",
    "f = open(\"./data/lm/train.txt\",'r+',encoding=\"utf-8\")\n",
    "lines = []\n",
    "s = set([\"<s>\",\"</s>\",\"<UNK>\"])\n",
    "sentences = []\n",
    "for line in f.readlines():\n",
    "    # some lines are still end with \\n, need to remove \\n\n",
    "    if line.endswith(\"\\n\"):\n",
    "        line = line[:-1]\n",
    "    tmp = line.split(' ')\n",
    "    sentences.append(tmp)\n",
    "    for word in tmp:\n",
    "        s.add(word)\n",
    "f.close()\n",
    "\n",
    "word_list = sorted(list(s))\n",
    "\n",
    "word_count = defaultdict(int)\n",
    "\n",
    "\n",
    "# count every word\n",
    "for sen in sentences:\n",
    "    for word in sen:\n",
    "        word_count[word]+=1\n",
    "\n",
    "# calculate <UNK>\n",
    "word_count_dic = {\"<UNK>\":0}\n",
    "for key in word_list:\n",
    "    value = word_count[key]\n",
    "    if value<3:\n",
    "        word_count_dic[\"<UNK>\"]+=1\n",
    "    else:\n",
    "        word_count_dic[key]=value\n",
    "\n",
    "word_list = sorted(word_count_dic.keys())\n",
    "word_dic = {key:idx for (idx,key) in enumerate(word_list)}\n",
    "tmp = []\n",
    "for key in word_dic:\n",
    "    tmp.append(word_count_dic[key])\n",
    "word_count = np.array(tmp)\n",
    "\n",
    "# change sentence to id list\n",
    "\n",
    "tmp_sentences = []\n",
    "for sentence in sentences:\n",
    "    tmp = []\n",
    "    for word in sentence:\n",
    "        if word in word_dic:\n",
    "            tmp.append(word_dic[word])\n",
    "        else:\n",
    "            tmp.append(word_dic[\"<UNK>\"])\n",
    "    tmp_sentences.append(tmp[:])\n",
    "sentences = tmp_sentences\n",
    "\n",
    "#vocabulary size\n",
    "print(\"vocabulary size: %d\"%len(word_list))\n",
    "\n",
    "del word_list\n",
    "del word_count_dic\n",
    "del tmp\n",
    "\n",
    "\n",
    "# word_dic : the map between words and id, \n",
    "# word_count : the number of each word organized with id\n",
    "# sentences : sentences in training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7oBATsX8uHb"
   },
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please show the vocabulary size and discuss the number of parameters of n-gram models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU6VpAkS9odh"
   },
   "source": [
    "The vocabulary size is 22,630, not including `<s>` and `</s>` characters. So, as for the number of parameters of n-gram models, when the n increases, the number of parameters increases sharply. For example, when n is 1 in this case, the number of parameters is 22,629 which is the vocabulary size. When n is 2, the vocabulary size is 512,116,900 which is the square of vocabulary size. For n equals 3, the vocabulary size is the cube of vocabulary size : 11,589,205,447,000. If we use a float variable to store a single probability, it would use around 84TB memory. So, the bigger n we have, more memory would be used by probability matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ2BGUig8TqH"
   },
   "source": [
    "### 1.2 $n$-gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing your vocabulary, you are expected to build bigram and unigram language models and report their perplexity on the training set, and dev set. Please discuss your experimental results. If you encounter any problems, please analyze them and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeyANMPe9ad_"
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ACSfNZGE8Yw2"
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "from collections import Counter\n",
    "import math\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram language model\n",
    "# as for n=1, I only need to calculate every words' count and then calculate the probability matrix\n",
    "# which is quite easy to write\n",
    "# the probability matrix would look like this:\n",
    "#           [P_0, P_1, P_2, P_3, ...... , P_n, P_start, P_end]\n",
    "#           P_start, P_end are the probability of start padding and end padding\n",
    "\n",
    "class UnigramModel:\n",
    "    def __init__(self, sentences,word_dic,word_count,file = \"\"):\n",
    "        if file!=\"\":\n",
    "            self.uniProb = np.load(file)\n",
    "        else:\n",
    "            self.uniProb = self.__cal_prob(sentence,word_dic,word_count)\n",
    "        self.word_dic = word_dic\n",
    "        \n",
    "    # calculate the probability array\n",
    "    def __cal_prob(self,sentences,word_dic,word_count):\n",
    "        startpad, endpad = len(sentences),len(sentences)\n",
    "        count = startpad+endpad\n",
    "        for nu in word_count:\n",
    "            count += nu\n",
    "\n",
    "        uniProb = [0.0] * (len(word_dic)+2)\n",
    "        for i in range(len(word_dic)):\n",
    "            uniProb[i] = word_count[i] / count\n",
    "        uniProb[-2] = startpad / count\n",
    "        uniProb[-1] = endpad / count\n",
    "        return np.array(uniProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model and save the results\n",
    "# uni = UnigramModel(sentences,word_dic,word_count)\n",
    "# np.save(\"./unigram.npy\",uni.uniProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni = UnigramModel(sentences,word_dic,word_count,file=\"./unigram.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram language model\n",
    "# the probability matrix would look like this:\n",
    "#    first word(i) /second word(j)\n",
    "#           [w_0, w_1, w_2, w_3, ...... , w_n, startpad, endpad]\n",
    "#     [w_0]\n",
    "#     ......\n",
    "#     [startpad]\n",
    "#     [endpad] this line is meaningless, deleted\n",
    "#                       matrix[i][j] = P(j|i)\n",
    "\n",
    "class BigramModel:\n",
    "    def __init__(self,sentences,word_dic,word_count,file = \"\"):\n",
    "        if file != \"\":\n",
    "            self.biProb = np.load(file)\n",
    "        else:\n",
    "            # self.uniProb = UnigramModel(sentences,word_dic,word_count).uniProb\n",
    "            self.biProb = self.__cal_prob(sentences,word_dic,word_count)\n",
    "        self.word_dic = word_dic\n",
    "    \n",
    "    # calculate the probability matrix\n",
    "    def __cal_prob(self,sentences,word_dic,word_count):\n",
    "        count_matrix = np.zeros((len(word_dic)+1,len(word_dic)+2),dtype=float)\n",
    "        startpad,endpad = len(word_dic),len(word_dic)+1\n",
    "        for sen in sentences:\n",
    "            for (before,after) in bigrams(sen,pad_left=True, pad_right=True, left_pad_symbol=startpad, right_pad_symbol=endpad):\n",
    "                count_matrix[before][after] += 1\n",
    "        \n",
    "        # add <s> at the end of word_count, which is equal to the number of sentences\n",
    "        word_count = np.append(word_count,np.array([len(sentences)]))\n",
    "        for i in range(len(count_matrix)):\n",
    "            for j in range(len(count_matrix[0])):\n",
    "                count_matrix[i][j] /= word_count[i]\n",
    "        return count_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the model and save it to disk\n",
    "bi = BigramModel(sentences,word_dic,word_count)    \n",
    "\n",
    "np.save(\"./bigram.npy\",bi.biProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model from file\n",
    "bi = BigramModel(sentences,word_dic,word_count,\"./bigram.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### calculate perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ppl of uni-gram model\n",
    "def uni_ppl(un:UnigramModel,filePath:string):\n",
    "    f = open(filePath,\"r\",encoding=\"utf-8\")\n",
    "    su = 0.0\n",
    "    unkidx = word_dic[\"<UNK>\"]\n",
    "    M = 0\n",
    "    for line in f.readlines():\n",
    "        M+=1\n",
    "        # cur state at <s> which is -1 index\n",
    "        cur = -1\n",
    "        p = 0.0\n",
    "        fail = False\n",
    "        for word in line.split(\" \"):\n",
    "            if word==\"\\n\":\n",
    "                continue\n",
    "            if word in word_dic:\n",
    "                su += math.log2(un.uniProb[word_dic[word]])\n",
    "            else:\n",
    "                su += math.log2(un.uniProb[unkidx])\n",
    "                \n",
    "        \n",
    "        # for </s> multiply this probability\n",
    "        su += math.log2(un.uniProb[-1])\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    l = su/M\n",
    "    # print(\"this is l\",l)\n",
    "    ppl = math.pow(2,-l)\n",
    "    print(\"the perplexity of this uni-gram model on %s is:%f\"%(filePath,ppl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of this uni-gram model on ./data/lm/train.txt is:5660301222045054989618096252612976439614536326114634894929397503688601222774784.000000\n",
      "the perplexity of this uni-gram model on ./data/lm/dev.txt is:317558791101424590076948643108686090543715386040142851437753083182687715328000.000000\n"
     ]
    }
   ],
   "source": [
    "un = UnigramModel(sentences,word_dic,word_count,\"./unigram.npy\")\n",
    "uni_ppl(un,\"./data/lm/train.txt\")\n",
    "uni_ppl(un,\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity of bi-gram model\n",
    "def bi_ppl(bi:BigramModel, filePath:string):\n",
    "    f = open(filePath,\"r\",encoding=\"utf-8\")\n",
    "    su = 0.0\n",
    "    unkidx = word_dic[\"<UNK>\"]\n",
    "    M = 0\n",
    "    for line in f.readlines():\n",
    "        M += 1\n",
    "        # cur state at <s> which is -1 index\n",
    "        cur = -1\n",
    "        # p = 1.0\n",
    "        p = 0.0\n",
    "        fail = False\n",
    "        for word in line.split(\" \"):\n",
    "            if word == \"\\n\":\n",
    "                continue\n",
    "            if word in word_dic :\n",
    "                nextstep = word_dic[word]\n",
    "                # p *= bi.biProb[cur][nextstep]\n",
    "                if bi.biProb[cur][nextstep] != 0:\n",
    "                    p += math.log2(bi.biProb[cur][nextstep])\n",
    "                    cur = nextstep\n",
    "                else:\n",
    "                    # if test fail, break \n",
    "                    fail = True\n",
    "                    break\n",
    "            else:\n",
    "                if bi.biProb[cur][unkidx] != 0:\n",
    "                    # p *= bi.biProb[cur][unkidx]\n",
    "                    p += math.log2(bi.biProb[cur][unkidx])\n",
    "                    cur = unkidx\n",
    "                else:\n",
    "                    # if test fail, break \n",
    "                    fail = True\n",
    "                    break\n",
    "        if not fail:\n",
    "            p += math.log2(bi.biProb[cur][-1]) # for </s> multiply this probability\n",
    "            su += p\n",
    "        else:\n",
    "            su += float(\"-inf\")\n",
    "    f.close()\n",
    "    l = su/M\n",
    "    ppl = math.pow(2,-l)\n",
    "    print(\"the perplexity of this bi-gram model on %s is:%f\"%(filePath,ppl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of this bi-gram model on ./data/lm/train.txt is:inf\n",
      "the perplexity of this bi-gram model on ./data/lm/dev.txt is:inf\n"
     ]
    }
   ],
   "source": [
    "bi = BigramModel(sentences,word_dic,word_count,\"./bigram.npy\")\n",
    "bi_ppl(bi,\"./data/lm/train.txt\")\n",
    "bi_ppl(bi,\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the first version of the program, p mutipled a lot times which leads to p equal to 0. As for the result, in the second version of the program, I change the way to calculate su.\n",
    "\n",
    "sometimes, biProb[i][j]==0, whichi means test fail, at that time perplexity of this sentence would be float('-inf')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRWC56a19TbY"
   },
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM4gcgL--Ylh"
   },
   "source": [
    "##### Problem Encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this process, I meet 2 diffient problems both related to how to calculate the perplexity. The one problem is occurred when calculate $p(x^{(i)})$, where $x^{(i)}$ is the $i^{th}$ sentence in the test set. The other problem happens when calculate the perplexity of bi-gran model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Question 1\n",
    "\n",
    "    The main issue of the first problem is mainly related to the represent ability of a float number, because Python uses float type to store decimal numbers. When a dicimal number is smaller than 0.00000001, the result of this number would be zero and this would lead to value error when I run log2 function. To resolve this problem, I use another way to calculate the perplexity. The definition of perplexity is listed below:\n",
    "    $$ l = {{{1} \\over {M}}  \\sum^{m}_{i=1} \\log_{2}{p(x^{(i)})}}  $$\n",
    "    $$ ppl = 2^{-l} $$\n",
    "    $$ {{p(x^{(i)})} = \\prod_{j=0}^n p(w_j|w_{j-1})} \\text{\\quad where $w_j$ is the $j^{th}$ word in $sentence_i$, n=len($sentence_i$)} $$ \n",
    "    Because if I calculate $p(x^{(i)})$ roughly, $p(x^{(i)})$ would be too small to store in a float type number. So I decide to use this   function to calculate $l$:\n",
    "    $$ l = {{{1} \\over {M}}  \\sum^{m}_{i=1} \\sum^{n}_{j=0} \\log_{2}{p(w_j|w_{j-1})}} $$\n",
    "    $$\\text{\\quad where $w_j$ is the $j^{th}$ word in $sentence_i$, $w_{-1}$=\"<s>\", n=len($sentence_i$)},m=len(sentences) $$\n",
    "    With this function, the result of calculating perplexity would not be too small, so that the perplexity can be calculated successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Question 2 \n",
    "\n",
    "    As for the second problem, it occored when $p(w_j|w_{j-1})$ equals 0 during testing bi-gram model. The mainly reason could be that the train set is not large enough to make sure that every possible $p(w_j|w_{j-1})$ is included. If a word set ($w_{j-1}$,$w_j$) is met and the probability $p(w_j|w_{j-1})$ equals zero, then the test prosess would be terminated because there is no probability to generate a sentence like the test sentence. \n",
    "\n",
    "    In this case, the $l$ should be self defigned in the program because calculating $\\log_{2}0$ is actually an error mathmatically. So in the actual program, when then program test a sentence failed because $p(w_j|w_{j-1})=0$, it is better to add a punishment to the attribute $l$, I choose to add $-\\infty$ as the result of $\\sum^{n}_{j=0} \\log_{2}{p(w_j|w_{j-1})}$ because $\\lim_{x\\rightarrow{0}}\\log_{2}x=-\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity of uni-gram model is around $5.66*10^{78}$ on training set and $3.1756*10^{77}$ on dev set, and the perplexity of the bi-gram model is inf on both two sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the uni-gram model, the perplexity is quight high, which means this model cannot predict the sentences in the set. I believe the main reason is that this model only is contained the probability of each words and every sentence is generated in a certain probability. So that this model cannot fit both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the bi-gram model, the perplexity is infinite, which means this model is terrible.  Actually, in the probability matrix generated by the bi-gram model, nearly 99.9% of the conditional probability are 0. This would caused by the size of training set, if the training set is not large enough, it is possible that the probability matrix calculated by the bi-gram model could not contains every possible two word set in the real world. Therefore, the perplexity could be infinite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the probability is 0 occupied: 99.901397%\n"
     ]
    }
   ],
   "source": [
    "le,wid = len(bi.biProb),len(bi.biProb[0])\n",
    "t = le*wid\n",
    "for i in range(le):\n",
    "    for j in range(wid):\n",
    "        if bi.biProb[i][j]!=0:\n",
    "            t-=1\n",
    "print(\"the probability is 0 occupied: %f%%\"%(t/le/wid*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I believe that there is some ways that can improve this model. Firstly, increasing the size of training data. Secondly, using smoothing to give little probability to every zero members in the probability matrix. Finally, combining the bi-gram model with the uni-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuLL8CH1Ua-3"
   },
   "source": [
    "### 1.3 Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7mWQhaCUixZ"
   },
   "source": [
    "#### 1.3.1 Add-one (Laplace) smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbbHxLDmVrz6"
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "93_yLu9dVr0C"
   },
   "outputs": [],
   "source": [
    "class AddOneBigramModel:\n",
    "    def __init__(self,sentences,word_dic,word_count,file = \"\"):\n",
    "        if file != \"\":\n",
    "            self.biProb = np.load(file)\n",
    "        else:\n",
    "            # self.uniProb = UnigramModel(sentences,word_dic,word_count).uniProb\n",
    "            self.biProb = self.__cal_prob(sentences,word_dic,word_count)\n",
    "        self.word_dic = word_dic\n",
    "    \n",
    "    # with add-one smoothing\n",
    "    def __cal_prob(self,sentences,word_dic,word_count):\n",
    "        # for the one in add one smoothing\n",
    "        count_matrix = np.ones((len(word_dic)+1,len(word_dic)+2),dtype=float)\n",
    "        startpad,endpad = len(word_dic),len(word_dic)+1\n",
    "        for sen in sentences:\n",
    "            for (before,after) in bigrams(sen,pad_left=True, pad_right=True, left_pad_symbol=startpad, right_pad_symbol=endpad):\n",
    "                count_matrix[before][after] += 1\n",
    "        \n",
    "        V = (len(word_dic)+1)*(len(word_dic)+2)\n",
    "        # add <s> at the end of word_count, which is equal to the number of sentences\n",
    "        word_count = np.append(word_count,np.array([len(sentences)]))\n",
    "\n",
    "        for i in range(len(count_matrix)):\n",
    "            for j in range(len(count_matrix[0])):\n",
    "                if count_matrix[i][j] != 0:\n",
    "                    count_matrix[i][j] /= (word_count[i]+V)\n",
    "        return count_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddOneSmoothing = AddOneBigramModel(sentences,word_dic,word_count)\n",
    "np.save(\"addone.npy\",AddOneSmoothing.biProb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of this bi-gram model on ./data/lm/train.txt is:95055124317624193503796477484793156486160582624866378468182748985267486234526725074561635241663719644681725636597291748489206675662318955703642398781204533517585706351331070067207045120.000000\n",
      "the perplexity of this bi-gram model on ./data/lm/dev.txt is:43493400357187701191559668078523968556486362760788402784535305773788099872142596247080124447594898984572116504211485085771544402832227329764943127220541798125272431795324745327046557696.000000\n"
     ]
    }
   ],
   "source": [
    "bi_ppl(AddOneSmoothing,\"./data/lm/train.txt\")\n",
    "bi_ppl(AddOneSmoothing,\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8y1WOQtsVr0D"
   },
   "source": [
    "##### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTknh9pRVr0D"
   },
   "source": [
    "The perplexity of this bi-gram model after add-one smoothing are $9.5*10^{184}$ on training data and $4.35*10^{184}$ on dev data.\n",
    "\n",
    "This is the function to calculate the perplexity with add-one smoothing:\n",
    "$$ P^{*}_{Add-one}(w_n|w_{n-1})={{c(w_n,w_{n-1})+1} \\over {c(w_{n-1})+V}}$$\n",
    "\n",
    "For every $P^{*}_{Add-one}(w_n|w_{n-1})$ that $c(w_n,w_{n-1})$ equals zero, the probability is $ {1} \\over {c(w_{n-1})+V}$ rather than 0, which means that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTC0qJE8VVha"
   },
   "source": [
    "##### Optional: Add-k smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3itGMOOVuNg"
   },
   "source": [
    "###### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jhcuJWo7VuNg"
   },
   "outputs": [],
   "source": [
    "class AddKBigramModel:\n",
    "    def __init__(self,sentences,word_dic,word_count,file = \"\",k:float =1.0):\n",
    "        if file != \"\":\n",
    "            self.biProb = np.load(file)\n",
    "        else:\n",
    "            # self.uniProb = UnigramModel(sentences,word_dic,word_count).uniProb\n",
    "            self.biProb = self.__cal_prob(sentences,word_dic,word_count,k)\n",
    "        self.word_dic = word_dic\n",
    "    \n",
    "    # with add-one smoothing\n",
    "    def __cal_prob(self,sentences,word_dic,word_count,k):\n",
    "        # for the k\n",
    "        count_matrix = np.full((len(word_dic)+1,len(word_dic)+2),k,dtype=float)\n",
    "        startpad,endpad = len(word_dic),len(word_dic)+1\n",
    "        for sen in sentences:\n",
    "            for (before,after) in bigrams(sen,pad_left=True, pad_right=True, left_pad_symbol=startpad, right_pad_symbol=endpad):\n",
    "                count_matrix[before][after] += 1\n",
    "        \n",
    "        V = (len(word_dic)+1)*(len(word_dic)+2)*k\n",
    "        # add <s> at the end of word_count, which is equal to the number of sentences\n",
    "        word_count = np.append(word_count,np.array([len(sentences)]))\n",
    "\n",
    "        for i in range(len(count_matrix)):\n",
    "            for j in range(len(count_matrix[0])):\n",
    "                if count_matrix[i][j] != 0:\n",
    "                    count_matrix[i][j] /= (word_count[i]+V)\n",
    "        return count_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try k=0.5, 0.05, 0.01 in the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "klist = [0.5,0.05,0.01]\n",
    "\n",
    "for k in klist:\n",
    "    t = AddKBigramModel(sentences,word_dic,word_count,k=k)\n",
    "    bi_ppl(AddOneSmoothing,\"./data/lm/train.txt\")\n",
    "    bi_ppl(AddOneSmoothing,\"./data/lm/dev.txt\")\n",
    "    del t # to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzpU_p6CVuNg"
   },
   "source": [
    "###### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIOUpNXYVuNh"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJFWxsN-Uj0Y"
   },
   "source": [
    "#### 1.3.2 Linear Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk11EpboWVCH"
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4N_XuN6WVCQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh8v2P36WVCQ"
   },
   "source": [
    "##### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGkf6IV0WVCQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvgTZcNFVato"
   },
   "source": [
    "##### Optional: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKnXu98hWcfu"
   },
   "source": [
    "###### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKo4ZLASWcfu"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E8xYXuCUoAR"
   },
   "source": [
    "## 2 Preposition Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gewisxM5W5kQ"
   },
   "outputs": [],
   "source": [
    "!wget -O dev.in https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.in\n",
    "!wget -O dev.out https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.out"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
