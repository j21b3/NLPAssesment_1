{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "## 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsSAtTqt7Q8a"
      },
      "outputs": [],
      "source": [
        "!wget -O train.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O dev.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O test.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "### 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You will download and preprocess the tokenized training data to build the vocabulary. To handle out-of-\n",
        "vocabulary(OOV) words, you will convert tokens that occur less than three times in the training data into\n",
        "a special unknown token 〈UNK〉. You should also add start-of-sentence tokens 〈s〉and end-of-sentence\n",
        "〈/s〉tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary size: 22629\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# So, build the model based on make the vocabulary set on training set\n",
        "f = open(\"./data/lm/train.txt\",'r+',encoding=\"utf-8\")\n",
        "lines = []\n",
        "s = set([\"<s>\",\"</s>\",\"<UNK>\"])\n",
        "sentences = []\n",
        "for line in f.readlines()[:-1]:\n",
        "    # some lines are still end with \\n, need to remove \\n\n",
        "    if line.endswith(\"\\n\"):\n",
        "        line = line[:-1]\n",
        "    tmp = line.split(' ')\n",
        "    sentences.append(tmp)\n",
        "    for word in tmp:\n",
        "        s.add(word)\n",
        "f.close()\n",
        "\n",
        "word_list = sorted(list(s))\n",
        "\n",
        "word_count = defaultdict(int)\n",
        "\n",
        "\n",
        "# count every word\n",
        "for sen in sentences:\n",
        "    for word in sen:\n",
        "        word_count[word]+=1\n",
        "\n",
        "# calculate <UNK>\n",
        "word_count_dic = {\"<UNK>\":0}\n",
        "for key in word_list:\n",
        "    value = word_count[key]\n",
        "    if value<3:\n",
        "        word_count_dic[\"<UNK>\"]+=1\n",
        "    else:\n",
        "        word_count_dic[key]=value\n",
        "\n",
        "word_list = sorted(word_count_dic.keys())\n",
        "word_dic = {key:idx for (idx,key) in enumerate(word_list)}\n",
        "tmp = []\n",
        "for key in word_dic:\n",
        "    tmp.append(word_count_dic[key])\n",
        "word_count = np.array(tmp)\n",
        "\n",
        "# change sentence to id list\n",
        "\n",
        "tmp_sentences = []\n",
        "for sentence in sentences:\n",
        "    tmp = []\n",
        "    for word in sentence:\n",
        "        if word in word_dic:\n",
        "            tmp.append(word_dic[word])\n",
        "        else:\n",
        "            tmp.append(word_dic[\"<UNK>\"])\n",
        "    tmp_sentences.append(tmp[:])\n",
        "sentences = tmp_sentences\n",
        "\n",
        "#vocabulary size\n",
        "print(\"vocabulary size: %d\"%len(word_list))\n",
        "\n",
        "del word_list\n",
        "del word_count_dic\n",
        "del tmp\n",
        "\n",
        "\n",
        "# word_dic : the map between words and id, \n",
        "# word_count : the number of each word organized with id\n",
        "# sentences : sentences in training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please show the vocabulary size and discuss the number of parameters of n-gram models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "The vocabulary size is 22,629, not including `<s>` and `</s>` characters. So, as for the number of parameters of n-gram models, when the n increases, the number of parameters increases sharply. For example, when n is 1 in this case, the number of parameters is 22,629 which is the vocabulary size. When n is 2, the vocabulary size is 512,071,641 which is the square of vocabulary size. For n equals 3, the vocabulary size is the cube of vocabulary size : 11,587,669,164,189. If we use a float variable to store a single probability, it would use 86,334.86303827912GB memory. So, the bigger n I have, more memory would be used by probability matrix. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "### 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After preparing your vocabulary, you are expected to build bigram and unigram language models and report their perplexity on the training set, and dev set. Please discuss your experimental results. If you encounter any problems, please analyze them and explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ACSfNZGE8Yw2"
      },
      "outputs": [],
      "source": [
        "from nltk.util import bigrams\n",
        "from collections import Counter\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# unigram language model\n",
        "# as for n=1, I only need to calculate every words' count and then calculate the probability matrix\n",
        "# which is quite easy to write\n",
        "# the probability matrix would look like this:\n",
        "#           [P_0, P_1, P_2, P_3, ...... , P_n, P_start, P_end]\n",
        "#           P_start, P_end are the probability of start padding and end padding\n",
        "\n",
        "class UnigramModel:\n",
        "    def __init__(self, sentences,word_dic,word_count,file = \"\"):\n",
        "        if file!=\"\":\n",
        "            self.uniProb = np.load(file)\n",
        "        else:\n",
        "            self.uniProb = self.__cal_prob(sentence,word_dic,word_count)\n",
        "        \n",
        "    # calculate the probability array\n",
        "    def __cal_prob(self,sentences,word_dic,word_count):\n",
        "        startpad, endpad = len(sentences),len(sentences)\n",
        "        count = startpad+endpad\n",
        "        for nu in word_count:\n",
        "            count += nu\n",
        "\n",
        "        uniProb = [0.0] * (len(word_dic)+2)\n",
        "        for i in range(len(word_dic)):\n",
        "            uniProb[i] = word_count[i] / count\n",
        "        uniProb[-2] = startpad / count\n",
        "        uniProb[-1] = endpad / count\n",
        "        return np.array(uniProb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "uni = UnigramModel(sentences,word_dic,word_count)\n",
        "np.save(\"./unigram.npy\",uni.uniProb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "uni = UnigramModel(sentences,word_dic,word_count,file=\"./unigram.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# bigram language model\n",
        "# the probability matrix would look like this:\n",
        "#    first word(i) /second word(j)\n",
        "#           [w_0, w_1, w_2, w_3, ...... , w_n, startpad, endpad]\n",
        "#     [w_0]\n",
        "#     ......\n",
        "#     [startpad]\n",
        "#     [endpad] this line is meanless, deleted\n",
        "#                       matrix[i][j] = P(j|i)\n",
        "\n",
        "class BigramModel:\n",
        "    def __init__(self,sentences,word_dic,word_count,file = \"\"):\n",
        "        if file != \"\":\n",
        "            self.biProb = np.load(file)\n",
        "        else:\n",
        "            self.uniProb = UnigramModel(sentences,word_dic,word_count).uniProb\n",
        "            self.biProb = self.__cal_prob(sentences,word_dic)\n",
        "    \n",
        "    # calculate the probability matrix\n",
        "    def __cal_prob(self,sentences,word_dic):\n",
        "        count_matrix = np.zeros((len(word_dic)+1,len(word_dic)+2),dtype=float)\n",
        "        startpad,endpad = len(word_dic),len(word_dic)+1\n",
        "        count = 0\n",
        "        for sen in sentences:\n",
        "            for (before,after) in bigrams(sen,pad_left=True, pad_right=True, left_pad_symbol=startpad, right_pad_symbol=endpad):\n",
        "                count_matrix[before][after] += 1\n",
        "                count += 1\n",
        "        # FIXME:计算时间过长\n",
        "        for i in range(len(count_matrix)):\n",
        "            for j in range(len(count_matrix[0])):\n",
        "                if count_matrix[i][j] != 0:\n",
        "                    count_matrix[i][j] /= count\n",
        "        return count_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the model and save it to disk\n",
        "bi = BigramModel(sentences,word_dic,word_count)    \n",
        "#print(list(bi.probability))\n",
        "np.save(\"./bigram.npy\",bi.biProb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read model from file\n",
        "bi = BigramModel(sentences,word_dic,word_count,\"./bigram.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "math domain error",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-ff7417608849>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mcur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munkidx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mbi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiProb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# for </s> multiply this probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0msu\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: math domain error"
          ]
        }
      ],
      "source": [
        "# calculate perplexity\n",
        "f = open(\"./data/lm/test.txt\",\"r\",encoding=\"utf-8\")\n",
        "su = 0.0\n",
        "unkidx = word_dic[\"<UNK>\"]\n",
        "M = 0\n",
        "for line in f.readlines():\n",
        "    M += 1\n",
        "    # cur state at <s> which is -1 index\n",
        "    cur = -1\n",
        "    p = 1\n",
        "    for word in line:\n",
        "        if word in word_dic:\n",
        "            nextstep = word_dic[word]\n",
        "            p *= bi.biProb[cur][nextstep]\n",
        "            cur = nextstep\n",
        "        else:\n",
        "            p *= bi.biProb[cur][unkidx]\n",
        "            cur = unkidx\n",
        "    p *= bi.biProb[cur][-1] # for </s> multiply this probability\n",
        "    print(p)\n",
        "    su += math.log2(p)\n",
        "f.close()\n",
        "l = su/M\n",
        "ppl = math.pow(2,-l)\n",
        "print(\"the perplexity of this bi-gram model is :%f\"%ppl)\n",
        "            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "### 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "#### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93_yLu9dVr0C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "##### Optional: Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "###### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "###### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "#### 1.3.2 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### Optional: Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "###### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E8xYXuCUoAR"
      },
      "source": [
        "## 2 Preposition Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gewisxM5W5kQ"
      },
      "outputs": [],
      "source": [
        "!wget -O dev.in https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.in\n",
        "!wget -O dev.out https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.out"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.6.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
