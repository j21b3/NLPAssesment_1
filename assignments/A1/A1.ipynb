{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI4A-T0z5AU6"
   },
   "source": [
    "# Assignment 1\n",
    "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEomoMzH5Nf6"
   },
   "source": [
    "## 1 $n$-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsSAtTqt7Q8a"
   },
   "outputs": [],
   "source": [
    "!wget -O train.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/train.txt\n",
    "!wget -O dev.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/dev.txt\n",
    "!wget -O test.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ElrINWW7oF7"
   },
   "source": [
    "### 1.1 Building vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eawcuVV19kZm"
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q-rNT_QL8Dvt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 20661\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# So, build the model based on make the vocabulary set on training set\n",
    "f = open(\"./data/lm/train.txt\",'r+',encoding=\"utf-8\")\n",
    "lines = []\n",
    "s = set([\"<s>\",\"</s>\",\"<UNK>\"])\n",
    "sentences = []\n",
    "for line in f.readlines():\n",
    "    # some lines are still end with \\n, need to remove \\n\n",
    "    line = line.strip()\n",
    "    tmp = line.split(' ')\n",
    "    sentences.append(tmp)\n",
    "    for word in tmp:\n",
    "        s.add(word.lower())\n",
    "f.close()\n",
    "\n",
    "word_list = sorted(list(s))\n",
    "\n",
    "word_count = defaultdict(int)\n",
    "\n",
    "\n",
    "# count every word\n",
    "for sen in sentences:\n",
    "    for word in sen:\n",
    "        word_count[word.lower()]+=1\n",
    "\n",
    "# calculate <UNK>\n",
    "word_count_dic = {\"<UNK>\":0}\n",
    "for key in word_list:\n",
    "    value = word_count[key]\n",
    "    if value<3:\n",
    "        word_count_dic[\"<UNK>\"]+=1\n",
    "    else:\n",
    "        word_count_dic[key]=value\n",
    "\n",
    "word_list = sorted(word_count_dic.keys())\n",
    "word_dic = {key:idx for (idx,key) in enumerate(word_list)}\n",
    "tmp = []\n",
    "for key in word_dic:\n",
    "    tmp.append(word_count_dic[key])\n",
    "word_count = np.array(tmp)\n",
    "\n",
    "# change sentence to id list\n",
    "\n",
    "tmp_sentences = []\n",
    "for sentence in sentences:\n",
    "    tmp = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word in word_dic:\n",
    "            tmp.append(word_dic[word])\n",
    "        else:\n",
    "            tmp.append(word_dic[\"<UNK>\"])\n",
    "    tmp_sentences.append(tmp[:])\n",
    "sentences = tmp_sentences\n",
    "\n",
    "#vocabulary size\n",
    "print(\"vocabulary size: %d\"%len(word_list))\n",
    "\n",
    "del word_list\n",
    "del word_count_dic\n",
    "del tmp\n",
    "\n",
    "\n",
    "# word_dic : the map between words and id, \n",
    "# word_count : the number of each word organized with id\n",
    "# sentences : sentences in training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7oBATsX8uHb"
   },
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please show the vocabulary size and discuss the number of parameters of n-gram models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU6VpAkS9odh"
   },
   "source": [
    "The vocabulary size is 20,661, not including `<s>` and `</s>` characters. So, as for the number of parameters of n-gram models, when the n increases, the number of parameters increases sharply. For example, when n is 1 in this case, the number of parameters is 22,629 which is the vocabulary size. When n is 2, the vocabulary size is 426,876,921 which is the square of vocabulary size. For n equals 3, the vocabulary size is the cube of vocabulary size : 11,589,205,447,000. If we use a float variable to store a single probability, it would use around 32TB memory. So, the bigger n we have, more memory would be used by probability matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ2BGUig8TqH"
   },
   "source": [
    "### 1.2 $n$-gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing your vocabulary, you are expected to build bigram and unigram language models and report their perplexity on the training set, and dev set. Please discuss your experimental results. If you encounter any problems, please analyze them and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeyANMPe9ad_"
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ACSfNZGE8Yw2"
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "from collections import Counter\n",
    "import math\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram language model\n",
    "# as for n=1, I only need to calculate every words' count and then calculate the probability matrix\n",
    "# which is quite easy to write\n",
    "# the probability matrix would look like this:\n",
    "#           [P_0, P_1, P_2, P_3, ...... , P_n, P_start, P_end]\n",
    "#           P_start, P_end are the probability of start padding and end padding\n",
    "\n",
    "class UnigramModel:\n",
    "    def __init__(self, sentences,word_dic,word_count,file = \"\"):\n",
    "        if file!=\"\":\n",
    "            self.uniProb = np.load(file)\n",
    "        else:\n",
    "            self.uniProb = self.__cal_prob(sentence,word_dic,word_count)\n",
    "        self.word_dic = word_dic\n",
    "        \n",
    "    # calculate the probability array\n",
    "    def __cal_prob(self,sentences,word_dic,word_count):\n",
    "        startpad, endpad = len(sentences),len(sentences)\n",
    "        count = startpad+endpad\n",
    "        for nu in word_count:\n",
    "            count += nu\n",
    "\n",
    "        uniProb = [0.0] * (len(word_dic)+2)\n",
    "        for i in range(len(word_dic)):\n",
    "            uniProb[i] = word_count[i] / count\n",
    "        uniProb[-2] = startpad / count\n",
    "        uniProb[-1] = endpad / count\n",
    "        return np.array(uniProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model and save the results\n",
    "uni = UnigramModel(sentences,word_dic,word_count)\n",
    "np.save(\"./unigram.npy\",uni.uniProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni = UnigramModel(sentences,word_dic,word_count,file=\"./unigram.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram language model\n",
    "# the probability matrix would look like this:\n",
    "#    first word(i) /second word(j)\n",
    "#           [w_0, w_1, w_2, w_3, ...... , w_n, startpad, endpad]\n",
    "#     [w_0]\n",
    "#     ......\n",
    "#     [startpad]\n",
    "#     [endpad] this line is meaningless, deleted\n",
    "#                       matrix[i][j] = P(j|i)\n",
    "\n",
    "class BigramModel:\n",
    "    def __init__(self,sentences,word_dic,word_count,file = \"\"):\n",
    "        if file != \"\":\n",
    "            self.biProb = np.load(file)\n",
    "        else:\n",
    "            # self.uniProb = UnigramModel(sentences,word_dic,word_count).uniProb\n",
    "            self.biProb = self.__cal_prob(sentences,word_dic,word_count)\n",
    "        self.word_dic = word_dic\n",
    "    \n",
    "    # calculate the probability matrix\n",
    "    def __cal_prob(self,sentences,word_dic,word_count):\n",
    "        count_matrix = np.zeros((len(word_dic)+1,len(word_dic)+2),dtype=float)\n",
    "        startpad,endpad = len(word_dic),len(word_dic)+1\n",
    "        for sen in sentences:\n",
    "            for (before,after) in bigrams(sen,pad_left=True, pad_right=True, left_pad_symbol=startpad, right_pad_symbol=endpad):\n",
    "                count_matrix[before][after] += 1\n",
    "        \n",
    "        # add <s> at the end of word_count, which is equal to the number of sentences\n",
    "        word_count = np.append(word_count,np.array([len(sentences)]))\n",
    "        for i in range(len(count_matrix)):\n",
    "            for j in range(len(count_matrix[0])):\n",
    "                count_matrix[i][j] /= word_count[i]\n",
    "        return count_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the model and save it to disk\n",
    "bi = BigramModel(sentences,word_dic,word_count)    \n",
    "\n",
    "np.save(\"./bigram.npy\",bi.biProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model from file\n",
    "bi = BigramModel(sentences,word_dic,word_count,\"./bigram.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### calculate perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ppl of uni-gram model\n",
    "def uni_ppl(un:UnigramModel,filePath:string):\n",
    "    f = open(filePath,\"r\",encoding=\"utf-8\")\n",
    "    su = 0.0\n",
    "    unkidx = word_dic[\"<UNK>\"]\n",
    "    M = 0\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        words = line.split(\" \")\n",
    "        \n",
    "        M += len(words)\n",
    "        fail = False\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in word_dic:\n",
    "                su += math.log2(un.uniProb[word_dic[word]])\n",
    "            else:\n",
    "                su += math.log2(un.uniProb[unkidx])\n",
    "                \n",
    "        \n",
    "        # for </s> multiply this probability\n",
    "        su += math.log2(un.uniProb[-1])\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    l = su/M\n",
    "    # print(\"this is l\",l)\n",
    "    ppl = math.pow(2,-l)\n",
    "    print(\"the perplexity of this uni-gram model on %s is:%f\"%(filePath,ppl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of this uni-gram model on ./data/lm/train.txt is:1342.968477\n",
      "the perplexity of this uni-gram model on ./data/lm/dev.txt is:1291.994081\n"
     ]
    }
   ],
   "source": [
    "un = UnigramModel(sentences,word_dic,word_count,\"./unigram.npy\")\n",
    "uni_ppl(un,\"./data/lm/train.txt\")\n",
    "uni_ppl(un,\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity of bi-gram model\n",
    "def bi_ppl(bi:BigramModel, filePath:string):\n",
    "    f = open(filePath,\"r\",encoding=\"utf-8\")\n",
    "    su = 0.0\n",
    "    unkidx = word_dic[\"<UNK>\"]\n",
    "    M = 0\n",
    "    for line in f.readlines():\n",
    "        line=line.strip()\n",
    "        words = line.split(\" \")\n",
    "        M += (len(words)+2)\n",
    "        # cur state at <s> which is -1 index\n",
    "        cur = -1\n",
    "        p = 0.0\n",
    "        fail = False\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word not in word_dic:\n",
    "                nextstep = unkidx\n",
    "            else:\n",
    "                nextstep = word_dic[word]\n",
    "            if bi.biProb[cur][nextstep]==0:\n",
    "                fail = True\n",
    "                break\n",
    "            else:\n",
    "                p += math.log2(bi.biProb[cur][nextstep])\n",
    "                cur = nextstep\n",
    "                \n",
    "        if not fail and bi.biProb[cur][-1]!=0:\n",
    "            p += math.log2(bi.biProb[cur][-1]) # for </s> multiply this probability\n",
    "            su += p\n",
    "        else:\n",
    "            su += float(\"-inf\")\n",
    "    f.close()\n",
    "    l = su/M\n",
    "    ppl = math.pow(2,-l)\n",
    "    print(\"the perplexity of this bi-gram model on %s is:%f\"%(filePath,ppl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of this bi-gram model on ./data/lm/train.txt is:60.886884\n",
      "the perplexity of this bi-gram model on ./data/lm/dev.txt is:inf\n"
     ]
    }
   ],
   "source": [
    "bi = BigramModel(sentences,word_dic,word_count,\"./bigram.npy\")\n",
    "bi_ppl(bi,\"./data/lm/train.txt\")\n",
    "bi_ppl(bi,\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRWC56a19TbY"
   },
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM4gcgL--Ylh"
   },
   "source": [
    "##### Problem Encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this process, I meet 2 diffient problems both related to how to calculate the perplexity. The one problem is occurred when calculate $p(x^{(i)})$, where $x^{(i)}$ is the $i^{th}$ sentence in the test set. The other problem happens when calculate the perplexity of bi-gran model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Question 1\n",
    "\n",
    "    The main issue of the first problem is mainly related to the represent ability of a float number, because Python uses float type to store decimal numbers. When a dicimal number is smaller than 0.00000001, the result of this number would be zero and this would lead to value error when I run log2 function. To resolve this problem, I use another way to calculate the perplexity. The definition of perplexity is listed below:\n",
    "    $$ l = {{{1} \\over {M}}  \\sum^{m}_{i=1} \\log_{2}{p(x^{(i)})}}  $$\n",
    "    $$ ppl = 2^{-l} $$\n",
    "    $$ {{p(x^{(i)})} = \\prod_{j=0}^n p(w_j|w_{j-1})} \\text{\\quad where $w_j$ is the $j^{th}$ word in $sentence_i$, n=len($sentence_i$)} $$ \n",
    "    Because if I calculate $p(x^{(i)})$ roughly, $p(x^{(i)})$ would be too small to store in a float type number. So I decide to use this   function to calculate $l$:\n",
    "    $$ l = {{{1} \\over {M}}  \\sum^{m}_{i=1} \\sum^{n}_{j=0} \\log_{2}{p(w_j|w_{j-1})}} $$\n",
    "    $$\\text{\\quad where $w_j$ is the $j^{th}$ word in $sentence_i$, $w_{-1}$=\"<s>\", n=len($sentence_i$)},m=len(sentences) $$\n",
    "    With this function, the result of calculating perplexity would not be too small, so that the perplexity can be calculated successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Question 2 \n",
    "\n",
    "    As for the second problem, it occored when $p(w_j|w_{j-1})$ equals 0 during testing bi-gram model. The mainly reason could be that the train set is not large enough to make sure that every possible $p(w_j|w_{j-1})$ is included. If a word set ($w_{j-1}$,$w_j$) is met and the probability $p(w_j|w_{j-1})$ equals zero, then the test prosess would be terminated because there is no probability to generate a sentence like the test sentence. \n",
    "\n",
    "    In this case, the $l$ should be self defigned in the program because calculating $\\log_{2}0$ is actually an error mathmatically. So in the actual program, when then program test a sentence failed because $p(w_j|w_{j-1})=0$, it is better to add a punishment to the attribute $l$, I choose to add $-\\infty$ as the result of $\\sum^{n}_{j=0} \\log_{2}{p(w_j|w_{j-1})}$ because $\\lim_{x\\rightarrow{0}}\\log_{2}x=-\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity of uni-gram model is around $1342.968477$ on training set and $1291.994081$ on dev set, and the perplexity of the bi-gram model is $60.886884$ on training set and $\\infty$ on dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the uni-gram model, the perplexity is quite high, which means this model cannot predict the sentences in the set. I believe the main reason is that this model only is contained the probability of each words and every sentence is generated in a certain probability. There are some uknown words in the dev set, in perplexity calculate process these words are be selected to <UNK> word, as a result the perplexity on dev set would be smaller than that on train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the bi-gram model, the perplexity on dev set is infinite, which means this model is terrible.  Actually, in the probability matrix generated by the bi-gram model, nearly 99.9% of the conditional probability are 0. This would caused by the size of training set, if the training set is not large enough, it is possible that the probability matrix calculated by the bi-gram model could not contains every possible two word set in the real world. Therefore, the perplexity could be infinite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the probability is 0 occupied: 99.885346%\n"
     ]
    }
   ],
   "source": [
    "le,wid = len(bi.biProb),len(bi.biProb[0])\n",
    "t = le*wid\n",
    "for i in range(le):\n",
    "    for j in range(wid):\n",
    "        if bi.biProb[i][j]!=0:\n",
    "            t-=1\n",
    "print(\"the probability is 0 occupied: %f%%\"%(t/le/wid*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I believe that there is some ways that can improve this model. Firstly, increasing the size of training data. Secondly, using smoothing to give little probability to every zero members in the probability matrix. Finally, combining the bi-gram model with the uni-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuLL8CH1Ua-3"
   },
   "source": [
    "### 1.3 Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7mWQhaCUixZ"
   },
   "source": [
    "#### 1.3.1 Add-one (Laplace) smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbbHxLDmVrz6"
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "93_yLu9dVr0C"
   },
   "outputs": [],
   "source": [
    "class AddOneBigramModel:\n",
    "    def __init__(self,sentences,word_dic,word_count,file = \"\"):\n",
    "        if file != \"\":\n",
    "            self.biProb = np.load(file)\n",
    "        else:\n",
    "            # self.uniProb = UnigramModel(sentences,word_dic,word_count).uniProb\n",
    "            self.biProb = self.__cal_prob(sentences,word_dic,word_count)\n",
    "        self.word_dic = word_dic\n",
    "    \n",
    "    # with add-one smoothing\n",
    "    def __cal_prob(self,sentences,word_dic,word_count):\n",
    "        # for the one in add one smoothing\n",
    "        count_matrix = np.ones((len(word_dic)+1,len(word_dic)+2),dtype=float)\n",
    "        startpad,endpad = len(word_dic),len(word_dic)+1\n",
    "        for sen in sentences:\n",
    "            for (before,after) in bigrams(sen,pad_left=True, pad_right=True, left_pad_symbol=startpad, right_pad_symbol=endpad):\n",
    "                count_matrix[before][after] += 1\n",
    "        \n",
    "        V = len(word_dic)+2\n",
    "        # add <s> at the end of word_count, which is equal to the number of sentences\n",
    "        word_count = np.append(word_count,np.array([len(sentences)]))\n",
    "\n",
    "        for i in range(len(count_matrix)):\n",
    "            for j in range(len(count_matrix[0])):\n",
    "                if count_matrix[i][j] != 0:\n",
    "                    count_matrix[i][j] /= (word_count[i]+V)\n",
    "        return count_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddOneSmoothing = AddOneBigramModel(sentences,word_dic,word_count)\n",
    "np.save(\"addone.npy\",AddOneSmoothing.biProb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of this bi-gram model on ./data/lm/train.txt is:616.761618\n",
      "the perplexity of this bi-gram model on ./data/lm/dev.txt is:701.757009\n"
     ]
    }
   ],
   "source": [
    "AddOneSmoothing = AddOneBigramModel(sentences,word_dic,word_count,file=\"./addone.npy\")\n",
    "bi_ppl(AddOneSmoothing,\"./data/lm/train.txt\")\n",
    "bi_ppl(AddOneSmoothing,\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8y1WOQtsVr0D"
   },
   "source": [
    "##### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTknh9pRVr0D"
   },
   "source": [
    "The perplexity of this bi-gram model after add-one smoothing are $616.761618$ on training data and $701.757009$ on dev data.\n",
    "\n",
    "This is the function to calculate the perplexity with add-one smoothing:\n",
    "$$ P^{*}_{Add-one}(w_n|w_{n-1})={{c(w_n,w_{n-1})+1} \\over {c(w_{n-1})+V}}$$\n",
    "\n",
    "For every $P^{*}_{Add-one}(w_n|w_{n-1})$ that $c(w_n,w_{n-1})$ equals zero, the probability is $ {1} \\over {c(w_{n-1})+V}$ rather than 0, which means that the model could also have the ability to predict those unseen sentences. Therefore, we can see from the result, the perplexity on the training data is increased significantly and that on the dev data is decreased a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTC0qJE8VVha"
   },
   "source": [
    "##### Optional: Add-k smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3itGMOOVuNg"
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jhcuJWo7VuNg"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class AddKBigramModel:\n",
    "    def __init__(self,sentences,word_dic,word_count,k:float =1.0):\n",
    "        \n",
    "        self.word_dic = word_dic\n",
    "        self.count1 = word_count\n",
    "        self.count1 = np.append(self.count1,[len(sentences),len(sentences)])\n",
    "        self.V = len(self.count1)\n",
    "        self.k = k\n",
    "        self.count2 = self.__count(sentences)\n",
    "    \n",
    "    # with add-one smoothing\n",
    "    def __count(self,sentences):\n",
    "        count = defaultdict(int)\n",
    "        startpad,endpad = len(word_dic),len(word_dic)+1\n",
    "\n",
    "        for sen in sentences:\n",
    "            for (before,after) in bigrams(sen,pad_left=True, pad_right=True, left_pad_symbol=startpad, right_pad_symbol=endpad):\n",
    "                count[(before,after)] += 1\n",
    "        return count\n",
    "    \n",
    "    def P(self,before,after):\n",
    "        c = self.count2[(before,after)]+self.k\n",
    "        \n",
    "        return c/(self.count1[before]+self.k*self.V)\n",
    "        \n",
    "    \n",
    "    def ppl(self,filePath = \"\"):\n",
    "        f = open(filePath,\"r\",encoding=\"utf-8\")\n",
    "        su = 0.0\n",
    "        unkidx = word_dic[\"<UNK>\"]\n",
    "        M = 0\n",
    "        start_idx = len(self.word_dic)\n",
    "        endidx = start_idx+1\n",
    "        \n",
    "        for line in f.readlines():\n",
    "            line=line.strip()\n",
    "            words = line.split(\" \")\n",
    "            M += (len(words)+2)\n",
    "            # cur state at <s> which is start_idx\n",
    "            cur = start_idx\n",
    "            p = 0.0\n",
    "            fail = False\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                if word not in word_dic:\n",
    "                    nextstep = unkidx\n",
    "                else:\n",
    "                    nextstep = word_dic[word]\n",
    "                try:\n",
    "                    p += math.log2(self.P(cur,nextstep))\n",
    "                except ValueError:\n",
    "                    fail = True\n",
    "                    break\n",
    "                cur = nextstep\n",
    "            if not fail and self.P(cur,endidx)!=0:\n",
    "                p += math.log2(self.P(cur,endidx)) # for </s> multiply this probability\n",
    "                su += p\n",
    "            else:\n",
    "                su += float(\"-inf\")\n",
    "        f.close()\n",
    "        l = su/M\n",
    "        ppl = math.pow(2,-l)\n",
    "        print(\"the perplexity of this bi-gram model on %s is:%f\"%(filePath,ppl))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try k=0.5, 0.05, 0.01 in the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 0.5\n",
      "the perplexity of this bi-gram model on ./data/lm/train.txt is:428.730279\n",
      "the perplexity of this bi-gram model on ./data/lm/dev.txt is:523.154802\n",
      "\n",
      "k= 0.05\n",
      "the perplexity of this bi-gram model on ./data/lm/train.txt is:152.856003\n",
      "the perplexity of this bi-gram model on ./data/lm/dev.txt is:253.474545\n",
      "\n",
      "k= 0.01\n",
      "the perplexity of this bi-gram model on ./data/lm/train.txt is:95.180805\n",
      "the perplexity of this bi-gram model on ./data/lm/dev.txt is:200.570883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "klist = [0.5,0.05,0.01]\n",
    "\n",
    "for k in klist:\n",
    "    print(\"k=\",k)\n",
    "    t = AddKBigramModel(sentences,word_dic,word_count,k=k)\n",
    "    t.ppl(\"./data/lm/train.txt\")\n",
    "    t.ppl(\"./data/lm/dev.txt\")\n",
    "    del t # to save memory\n",
    "    print()\n",
    "# t = AddKBigramModel(sentences,word_dic,word_count,k=0.001)\n",
    "# t.ppl(\"./data/lm/train.txt\")\n",
    "# t.ppl(\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzpU_p6CVuNg"
   },
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIOUpNXYVuNh"
   },
   "source": [
    "||k=1(Add-One smoothing)|k=0.5|k=0.05|k=0.01|k=0(Original Bigram Model)|\n",
    "|----------------|--------------|-----------------|-----------------|-----------|------|\n",
    "|ppl on train set|$616.761618$|$428.730279$|$152.856003$|$95.180805$|$60.886884$|\n",
    "|ppl on dev set|$701.757009$|$523.154802$|$253.474545$|$200.570883$|$\\infty$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the data shown in the table above, the smaller k is, the lower perplexity is and the better model I have,especially when k=0.01. In bigram model, the perplexity is extremely high because in most cases the predict process is failed as the conditional probability is zero. After add-one smoothing, the algorithm move some probability to the zero part. But it seems that this probability is so big that it interferes the normal predict process. Hence, add-k smoothing decreases the moved probability by using a parameter k to make sure that every conditional probability could have a minimun probability which is ${{k} \\over {c(w_{n-1})+kV}}$. And in this case, the model works best when $k=0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJFWxsN-Uj0Y"
   },
   "source": [
    "#### 1.3.2 Linear Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk11EpboWVCH"
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K4N_XuN6WVCQ"
   },
   "outputs": [],
   "source": [
    "# a probability of a word is r1P1+r2P2+r3P3\n",
    "from nltk.util import trigrams\n",
    "from collections import defaultdict\n",
    "\n",
    "class LinearNGram:\n",
    "    def __init__(self, sentences:list, word_dic:dict,word_count:np.array,\n",
    "                r1,r2,r3) -> None:\n",
    "        self.word_dic = word_dic\n",
    "        R = r1+r2+r3\n",
    "        self.r = [r1/R,r2/R,r3/R]\n",
    "\n",
    "        self.count_matrix1 = defaultdict(int)\n",
    "        self.sum1 = sum(word_count)\n",
    "        for (id,num) in enumerate(word_count):\n",
    "            self.count_matrix1[id]=num        \n",
    "        self.V2 = len(self.count_matrix1)+2\n",
    "        self.V3 = self.V2**2\n",
    "\n",
    "        self.count_matrix2 = defaultdict(int)\n",
    "        self.count_matrix3 = defaultdict(int)\n",
    "        self.__genCount(sentences)\n",
    "    \n",
    "    def __genCount(self,sentences) :\n",
    "        startpad,endpad = len(self.word_dic),len(self.word_dic)+1\n",
    "\n",
    "        self.count_matrix1[startpad] = len(sentences)\n",
    "        self.count_matrix1[endpad] = len(sentences)\n",
    "\n",
    "        self.count_matrix2[(startpad,startpad)] = len(sentences)\n",
    "\n",
    "        for sen in sentences:\n",
    "            for (before,after) in bigrams(sen,pad_left=True, pad_right=True, left_pad_symbol=startpad, right_pad_symbol=endpad):\n",
    "                self.count_matrix2[(before,after)] += 1\n",
    "            for (before1,before2,after) in trigrams(sen,pad_left=True, pad_right=True, left_pad_symbol=startpad, right_pad_symbol=endpad):\n",
    "                self.count_matrix3[(before1,before2,after)] += 1\n",
    "\n",
    "    def _P1(self,word):\n",
    "        return self.count_matrix1[word]/self.sum1\n",
    "    \n",
    "    def _P2(self, w_before,w_after):\n",
    "        return self.count_matrix2[(w_before,w_after)]/self.count_matrix1[w_before]\n",
    "    \n",
    "    def _P3(self, w_before1,w_before2,w_after):\n",
    "        if self.count_matrix2[(w_before1,w_before2)] == 0:\n",
    "            return 0 \n",
    "        else:\n",
    "            return self.count_matrix3[(w_before1,w_before2,w_after)]/self.count_matrix2[(w_before1,w_before2)]\n",
    "\n",
    "    def P(self,w_before1,w_before2,w_after):\n",
    "        return self.r[0]*self._P1(w_after) + self.r[1]*self._P2(w_before2,w_after) + \\\n",
    "            self.r[2]*self._P3(w_before1,w_before2,w_after)\n",
    "    \n",
    "    def changeR(self,r1,r2,r3):\n",
    "        R = r1+r2+r3\n",
    "        self.r = [r1/R,r2/R,r3/R]\n",
    "\n",
    "    def cal_ppl(self,filePath = \"\"):\n",
    "        f = open(filePath,'r',encoding='utf-8')\n",
    "        unkidx = word_dic[\"<UNK>\"]\n",
    "        su = 0.0\n",
    "        M = 0\n",
    "        start_status = [len(self.word_dic)]*2\n",
    "        end_idx = len(self.word_dic)+1\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()    # remove \\n\n",
    "            words = line.split(\" \")\n",
    "            M += (len(words)+4)\n",
    "            p = 0.0\n",
    "            before = start_status\n",
    "            for word in line.split(\" \"):\n",
    "                word = word.lower()\n",
    "                if word in self.word_dic:\n",
    "                    nextstep = self.word_dic[word]\n",
    "                else:\n",
    "                    nextstep = unkidx\n",
    "                \n",
    "                P = self.P(before[0],before[1],nextstep)\n",
    "                p += math.log2(P)\n",
    "                before[0],before[1] = before[1],nextstep\n",
    "            \n",
    "            # (word_n-1,word_n,end)\n",
    "            p += math.log2(self.P(before[0],before[1],end_idx))\n",
    "            # (word_n,end,end)\n",
    "            p += math.log2(self.P(before[1],end_idx,end_idx))\n",
    "            su += p\n",
    "        f.close()\n",
    "        l = su/M\n",
    "        ppl = math.pow(2,-l)\n",
    "        print(\"the perplexity of Linear Interpolation on %s is:%f\"%(filePath,ppl))\n",
    "        return ppl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:16.259040\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:109.440165\n"
     ]
    }
   ],
   "source": [
    "lin = LinearNGram(sentences,word_dic,word_count,1,1,1)\n",
    "_ = lin.cal_ppl(\"./data/lm/train.txt\")\n",
    "_ = lin.cal_ppl(\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:16.454264\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:108.226245\n"
     ]
    }
   ],
   "source": [
    "lin.changeR(0.2,0.5,0.3)\n",
    "_ = lin.cal_ppl(\"./data/lm/train.txt\")\n",
    "_ = lin.cal_ppl(\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:21.097855\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:109.763287\n"
     ]
    }
   ],
   "source": [
    "lin.changeR(0.4,0.4,0.2)\n",
    "_ = lin.cal_ppl(\"./data/lm/train.txt\")\n",
    "_ = lin.cal_ppl(\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:14.748860\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:111.143467\n"
     ]
    }
   ],
   "source": [
    "lin.changeR(0.3,0.3,0.4)\n",
    "_ = lin.cal_ppl(\"./data/lm/train.txt\")\n",
    "_ = lin.cal_ppl(\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:170.210268\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:308.730019\n"
     ]
    }
   ],
   "source": [
    "lin.changeR(0.99,0.005 ,0.005)\n",
    "_ = lin.cal_ppl(\"./data/lm/train.txt\")\n",
    "_ = lin.cal_ppl(\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:54.929393\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:227.670777\n"
     ]
    }
   ],
   "source": [
    "lin.changeR(0.005,0.99,0.005)\n",
    "_ = lin.cal_ppl(\"./data/lm/train.txt\")\n",
    "_ = lin.cal_ppl(\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.944311\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:709.194908\n"
     ]
    }
   ],
   "source": [
    "lin.changeR(0.005,0.005,0.99)\n",
    "_ = lin.cal_ppl(\"./data/lm/train.txt\")\n",
    "_ = lin.cal_ppl(\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:12.848475\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:131.527135\n"
     ]
    }
   ],
   "source": [
    "lin.changeR(0.05,0.45,0.5)\n",
    "_ = lin.cal_ppl(\"./data/lm/train.txt\")\n",
    "_ = lin.cal_ppl(\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:16.900118\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:107.828938\n"
     ]
    }
   ],
   "source": [
    "lin.changeR(0.3,0.4,0.3)\n",
    "_ = lin.cal_ppl(\"./data/lm/train.txt\")\n",
    "_ = lin.cal_ppl(\"./data/lm/dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh8v2P36WVCQ"
   },
   "source": [
    "##### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGkf6IV0WVCQ"
   },
   "source": [
    "When R=[0.3,0.4,0.3], the perplexity on train set and dev set are smallest, which are:$16.900118$ and $107.828938$ respectively. Actually, during my test, I find that the perplexity on train set is mainly influenced by trigram model and bigram model, while the trigram model seems to have a more significant effect on that. And unigram model seems to have a more significant effect on the perplexity on dev set, which means that unigram have a effect on predicting unseen words. With the limitation of bigram model, the perplexity on dev set could decrease to 114.909482 in the case R=[0.3,0.4,0.3]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvgTZcNFVato"
   },
   "source": [
    "##### Optional: Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNGramWIthGrad(LinearNGram):\n",
    "    def __init__(self, sentences:list, word_dic:dict,word_count:np.array,r1,r2,r3,learning_rate:float = 0.00001):\n",
    "        super(LinearNGramWIthGrad, self).__init__(sentences, word_dic, word_count, r1,r2,r3)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.partial = [0.0,0.0,0.0]\n",
    "    \n",
    "    def __addpartical(self,w_before1,w_before2,w_after):\n",
    "        P = self.P(w_before1,w_before2,w_after)\n",
    "        ln2 = math.log(2)\n",
    "        P1=self._P1(w_after)\n",
    "        P2=self._P2(w_before2,w_after)\n",
    "        P3=self._P3(w_before1,w_before2,w_after)\n",
    "        self.partial[0]+=((P1-P2)/P/ln2)\n",
    "        self.partial[1]+=((P2-P3)/P/ln2)\n",
    "        self.partial[2]+=((P3-P1)/P/ln2)\n",
    "    \n",
    "    def calDescent(self,ppl,M):\n",
    "        tmp = ppl*(-1/M)*math.log(2)*self.learning_rate\n",
    "        descent = [tmp*each for each in self.partial]\n",
    "        self.partial = [0.0,0.0,0.0]\n",
    "        return descent\n",
    "    \n",
    "    def adjustRwithDesc(self,descent):\n",
    "        print(\"Original R:\",self.r,end=\"--->\")\n",
    "        self.r[0] -= descent[0]\n",
    "        self.r[1] -= descent[1]\n",
    "        self.r[2] -= descent[2]\n",
    "        R = sum(self.r)\n",
    "        for i in range(3):\n",
    "            self.r[i] /= R\n",
    "        print(self.r)\n",
    "    \n",
    "    def cal_ppl(self,filePath = \"\"):\n",
    "        f = open(filePath,'r',encoding='utf-8')\n",
    "        unkidx = word_dic[\"<UNK>\"]\n",
    "        su = 0.0\n",
    "        M = 0\n",
    "        start_status = [len(self.word_dic)]*2\n",
    "        end_idx = len(self.word_dic)+1\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()    # remove \\n\n",
    "            words = line.split(\" \")\n",
    "            M += (len(words)+4)\n",
    "            p = 0.0\n",
    "            before = start_status\n",
    "            for word in line.split(\" \"):\n",
    "                word = word.lower()\n",
    "                if word in self.word_dic:\n",
    "                    nextstep = self.word_dic[word]\n",
    "                else:\n",
    "                    nextstep = unkidx\n",
    "                \n",
    "                prop = self.P(before[0],before[1],nextstep)\n",
    "                p += math.log2(prop)\n",
    "\n",
    "                self.__addpartical(before[0],before[1],nextstep)\n",
    "\n",
    "                before[0],before[1] = before[1],nextstep\n",
    "            \n",
    "            # (word_n-1,word_n,end)\n",
    "            p += math.log2(self.P(before[0],before[1],end_idx))\n",
    "            self.__addpartical(before[0],before[1],end_idx)\n",
    "            # (word_n,end,end)\n",
    "            p += math.log2(self.P(before[1],end_idx,end_idx))\n",
    "            self.__addpartical(before[1],end_idx,end_idx)\n",
    "            su += p\n",
    "        f.close()\n",
    "        l = su/M\n",
    "        ppl = math.pow(2,-l)\n",
    "        print(\"the perplexity of Linear Interpolation on %s is:%f\"%(filePath,ppl))\n",
    "\n",
    "        descent = self.calDescent(ppl,M)\n",
    "        print(\"descent gradient descent search result:\", descent)\n",
    "    \n",
    "        return (ppl,descent)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:107.828938\n",
      "descent gradient descent search result: [0.15601839878983037, -0.17813680738560103, 0.022118408595770973]\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:16.900118\n",
      "descent gradient descent search result: [0.05309769626447645, 0.2630433320565012, -0.3161410283220823]\n",
      "Turn  0\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:16.900118\n",
      "descent gradient descent search result: [0.05309769626447645, 0.2630433320565012, -0.3161410283220823]\n",
      "Original R: [0.3, 0.4, 0.3]--->[0.2469023037352508, 0.13695666794334752, 0.6161410283214016]\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:129.821888\n",
      "descent gradient descent search result: [0.9711665641411176, -1.7783583689473146, 0.8071918048059038]\n",
      "Turn  1\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:11.866648\n",
      "descent gradient descent search result: [0.036861793066130855, 0.08333531036534615, -0.12019710343122825]\n",
      "Original R: [0.2469023037352508, 0.13695666794334752, 0.6161410283214016]--->[0.21004051066917223, 0.053621357578014714, 0.7363381317528132]\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:158.707537\n",
      "descent gradient descent search result: [3.283011387049732, -5.003709718822969, 1.720698331775151]\n",
      "Turn  2\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.952774\n",
      "descent gradient descent search result: [0.0486497308373022, 0.04041043640629552, -0.08906016724347687]\n",
      "Original R: [0.21004051066917223, 0.053621357578014714, 0.7363381317528132]--->[0.1613907798318895, 0.01321092117172079, 0.8253982989963897]\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:208.456365\n",
      "descent gradient descent search result: [14.11446495460694, -18.23642827944774, 4.1219633248232395]\n",
      "Turn  3\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.528677\n",
      "descent gradient descent search result: [0.10632162744311051, -0.04150524386497653, -0.06481638357843335]\n",
      "Original R: [0.1613907798318895, 0.01321092117172079, 0.8253982989963897]--->[0.055069152388762525, 0.05471616503668095, 0.8902146825745566]\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:217.209631\n",
      "descent gradient descent search result: [0.6460836711448322, -9.700937096201761, 9.054853425056853]\n",
      "Turn  4\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.025064\n",
      "descent gradient descent search result: [0.0003696989421305761, 0.013655436939386607, -0.01402513588149225]\n",
      "Original R: [0.055069152388762525, 0.05471616503668095, 0.8902146825745566]--->[0.05469945344663331, 0.041060728097295364, 0.9042398184560714]\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:233.014384\n",
      "descent gradient descent search result: [2.877121997905894, -13.24969888901609, 10.372576891109322]\n",
      "Turn  5\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.014919\n",
      "descent gradient descent search result: [0.011083658422926579, -0.0005704650614648502, -0.010513193361450658]\n",
      "Original R: [0.05469945344663331, 0.041060728097295364, 0.9042398184560714]--->[0.043615795023707216, 0.04163119315876068, 0.9147530118175321]\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:245.207990\n",
      "descent gradient descent search result: [1.1657453610390924, -14.4422200802765, 13.276474719231574]\n",
      "Turn  6\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.012321\n",
      "descent gradient descent search result: [-0.0044556762034467885, -0.0027482974095807823, 0.007203973613029323]\n",
      "Original R: [0.043615795023707216, 0.04163119315876068, 0.9147530118175321]--->[0.048071471227154085, 0.04437949056834154, 0.9075490382045044]\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:235.930744\n",
      "descent gradient descent search result: [1.3405580546575016, -12.920753446080191, 11.580195391415446]\n",
      "Turn  7\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.011023\n",
      "descent gradient descent search result: [-0.00033302090300988004, 0.0019104182904005008, -0.0015773973873918251]\n",
      "Original R: [0.048071471227154085, 0.04437949056834154, 0.9075490382045044]--->[0.0484044921301639, 0.042469072277940986, 0.909126435591895]\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:238.066539\n",
      "descent gradient descent search result: [1.788706495600223, -13.50796981211712, 11.719263316525629]\n",
      "Turn  8\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.010926\n",
      "descent gradient descent search result: [0.0019968542077169883, -0.0003589117612412096, -0.0016379424464764744]\n",
      "Original R: [0.0484044921301639, 0.042469072277940986, 0.909126435591895]--->[0.046407637922446884, 0.04282798403918217, 0.910764378038371]\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:239.970531\n",
      "descent gradient descent search result: [1.398430297267908, -13.634923213444806, 12.23649291618917]\n",
      "Turn  9\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.010910\n",
      "descent gradient descent search result: [-0.001237024929067227, -0.0004235110484117314, 0.0016605359774753544]\n",
      "Original R: [0.046407637922446884, 0.04282798403918217, 0.910764378038371]--->[0.04764466285151395, 0.04325149508759375, 0.9091038420608923]\n",
      "the perplexity of Linear Interpolation on ./data/lm/dev.txt is:237.911046\n",
      "descent gradient descent search result: [1.505993833096274, -13.337135104218884, 11.831141271117374]\n",
      "-------------------------------------------------\n",
      "Best Turn 0\n",
      "ppl on train set 16.900118\n",
      "ppl on dev set 107.828938\n",
      "hyper parameter is  [0.3, 0.4, 0.3]\n",
      "\n",
      "\n",
      "Test on test set\n",
      "the perplexity of Linear Interpolation on ./data/lm/test.txt is:107.914203\n",
      "descent gradient descent search result: [0.154654970692217, -0.1753354500299286, 0.02068047933770929]\n",
      "ppl on test set is: 107.914203\n"
     ]
    }
   ],
   "source": [
    "def  gradientDescent(sentences,word_dic,word_count,r1,r2,r3,learning_rate,times):\n",
    "\n",
    "    grad = LinearNGramWIthGrad(sentences,word_dic,word_count,r1,r2,r3,learning_rate)\n",
    "    bestR = [r1,r2,r3]\n",
    "    minppldev,_ = grad.cal_ppl(\"./data/lm/dev.txt\")\n",
    "    ppltrain ,_ = grad.cal_ppl(\"./data/lm/train.txt\")\n",
    "    mul_ppl = (minppldev+ppltrain)/2\n",
    "    grad.partial = [0.0]*3\n",
    "    bestT = 0\n",
    "    for i in range(times):\n",
    "        print(\"Turn \",i,)\n",
    "        ppl,descent = grad.cal_ppl(\"./data/lm/train.txt\")\n",
    "        grad.adjustRwithDesc(descent)\n",
    "        ppl_dev,_ = grad.cal_ppl(\"./data/lm/dev.txt\")\n",
    "        grad.partial = [0.0]*3\n",
    "        if (ppl_dev+ppl)/2<mul_ppl:\n",
    "            bestT = i\n",
    "            bestR = grad.r[:]\n",
    "            minppldev = ppl_dev\n",
    "            ppltrain = ppl\n",
    "            mul_ppl = (minppldev+ppltrain)/2\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Best Turn %d\\n\\rppl on train set %f\\n\\rppl on dev set %f\\n\\rhyper parameter is \"%(bestT,ppltrain,minppldev),bestR)\n",
    "    print(\"\\n\\nTest on test set\")\n",
    "    grad.changeR(bestR[0],bestR[1],bestR[2])\n",
    "    ppl,_ = grad.cal_ppl(\"./data/lm/test.txt\")\n",
    "    print(\"ppl on test set is: %f\" % ppl)\n",
    "\n",
    "gradientDescent(sentences,word_dic,word_count,0.3,0.4,0.3,0.01,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKnXu98hWcfu"
   },
   "source": [
    "##### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKo4ZLASWcfu"
   },
   "source": [
    "Three are several ways to get the local optimum paramaters like parametric sweep, gradient descent search, or using $\\lambda$ matrix. Here I would like to use gradient descent in my model.\n",
    "Actually, the function to calculate the perplexity is:\n",
    "$$ ppl = 2^{-l} $$\n",
    "$$ l = {{{1} \\over {M}}  \\sum^{m}_{i=1} \\sum^{n}_{j=0} \\log_{2}{p^{*}(w_j|w_{j-1},w_{j-2})}} $$\n",
    "$$ p^{*}(w_j|w_{j-1},w_{j-2}) = \\lambda_1P(w_j)+\\lambda_2P(w_j|w_{j-1})+ \\lambda_3P(w_j|w_{j-1},w_{j-2})$$ \n",
    "$$ \\lambda_1+\\lambda_2+\\lambda_3=1 $$\n",
    "So, we can get the partial derivative of perplexity about $\\lambda_1,\\lambda_2,\\lambda_3$ like this:\n",
    "$$ {{\\partial ppl} \\over {\\partial \\lambda_1}} = -{{1}\\over M}*ppl*\\log 2*\\sum^{m}_{i=1} \\sum^{n}_{j=0} {{P(w_j)-P(w_j|w_{j-1})} \\over {p^{*}(w_j|w_{j-1},w_{j-2})*\\log 2}} $$\n",
    "$$ {{\\partial ppl} \\over {\\partial \\lambda_2}} = -{{1}\\over M}*ppl*\\log 2*\\sum^{m}_{i=1} \\sum^{n}_{j=0} {{P(w_j|w_{j-1})-P(w_j|w_{j-1},w_{j-2})} \\over {p^{*}(w_j|w_{j-1},w_{j-2})*\\log 2}} $$\n",
    "$$ {{\\partial ppl} \\over {\\partial \\lambda_3}} = -{{1}\\over M}*ppl*\\log 2*\\sum^{m}_{i=1} \\sum^{n}_{j=0} {{P(w_j|w_{j-1},w_{j-2})-P(w_j)} \\over {p^{*}(w_j|w_{j-1},w_{j-2})*\\log 2}} $$\n",
    "And it is easy to know that ${{\\partial ppl} \\over {\\partial \\lambda_1}}+{{\\partial ppl} \\over {\\partial \\lambda_2}}+{{\\partial ppl} \\over {\\partial \\lambda_3}}=0$, which can make sure that the adjustment would not break the normalization. The adjust functional is listed below:\n",
    "$$ \\lambda_1^{,} = \\lambda_1-LearningRate*{{\\partial ppl} \\over {\\partial \\lambda_1}} $$\n",
    "$$ \\lambda_2^{,} = \\lambda_2-LearningRate*{{\\partial ppl} \\over {\\partial \\lambda_2}} $$\n",
    "$$ \\lambda_3^{,} = \\lambda_3-LearningRate*{{\\partial ppl} \\over {\\partial \\lambda_3}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the examination of the gradient descent process, the perplexity on training set decreases in every turn and the perplexity on the test set have a different trend. So at last I choose a group of superparameters that has the lowest perplexity on test set. The hyperparameters I choose are $[0.2589453005798403, 0.23756411382677395, 0.5034905855933857]$.\n",
    "<!!!!>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E8xYXuCUoAR"
   },
   "source": [
    "## 2 Preposition Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gewisxM5W5kQ"
   },
   "outputs": [],
   "source": [
    "!wget -O dev.in https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.in\n",
    "!wget -O dev.out https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def predict(model :LinearNGramWIthGrad,FilePath:string):\n",
    "    word_predict = [\"at\",\"in\",\"of\",\"on\",\"for\"]\n",
    "    f = open(FilePath,\"r\",encoding=\"utf-8\")\n",
    "    startpad,endpad = len(model.word_dic),len(model.word_dic)+1\n",
    "    ret = []\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        words = line.split(\" \")\n",
    "        \n",
    "        line_pre = []\n",
    "        for (idx,w) in enumerate(words):\n",
    "            if w != \"<PREP>\":\n",
    "                continue\n",
    "            w = w.lower()\n",
    "            before1,before2 = 0,0\n",
    "            if idx == 0:\n",
    "                before1,before2 = startpad,startpad\n",
    "            elif idx ==1:\n",
    "                if words[idx-1].lower() in model.word_dic:\n",
    "                    before1,before2 = startpad,model.word_dic[words[idx-1].lower()]\n",
    "                else:\n",
    "                    before1,before2 = startpad,model.word_dic[\"<UNK>\"]\n",
    "            else:\n",
    "                if words[idx-2].lower() in model.word_dic:\n",
    "                    before1 = model.word_dic[words[idx-2].lower()]\n",
    "                else:\n",
    "                    before1 = model.word_dic[\"<UNK>\"]\n",
    "                if words[idx-1].lower() in model.word_dic:\n",
    "                    before2 = model.word_dic[words[idx-1].lower()]\n",
    "                else:\n",
    "                    before2 = model.word_dic[\"<UNK>\"]\n",
    "            predic = \"\"\n",
    "            # choose max P\n",
    "            maxp = float('-inf')\n",
    "            for wp in word_predict:\n",
    "                if maxp < model.P(before1,before2,model.word_dic[wp]):\n",
    "                    # print(before1,before2,wp)\n",
    "                    maxp = model.P(before1,before2,model.word_dic[wp])\n",
    "                    predic = wp\n",
    "\n",
    "            # random choose\n",
    "            # weight = [model.P(before1,before2,model.word_dic[wp]) for wp in word_predict]\n",
    "            # predic = random.choices(word_predict,weight,k=1)[0]\n",
    "            \n",
    "            line_pre.append(predic)\n",
    "        ret.append(line_pre[:])\n",
    "    f.close()\n",
    "    return ret\n",
    "\n",
    "def cal_correct(model :LinearNGramWIthGrad,FilePath:string, OutPath:string):\n",
    "    preResult = predict(model,FilePath)\n",
    "    f = open(OutPath,'r',encoding = 'utf-8')\n",
    "    i = 0\n",
    "    total_num = 0\n",
    "    correct_num = 0\n",
    "    for (idx,line) in enumerate(f.readlines()):\n",
    "        line = line.strip()\n",
    "        words = line.split(\" \")\n",
    "        if len(words)!=len(preResult[idx]):\n",
    "            raise Exception(\"Invalaid input: number of input incorrect\")\n",
    "        for i in range(len(words)):\n",
    "            total_num += 1\n",
    "            if words[i] == preResult[idx][i]:\n",
    "                correct_num +=1\n",
    "    print(\"correct rate:\",correct_num/total_num)\n",
    "    f.close()\n",
    "    return correct_num/total_num\n",
    "\n",
    "def saveFile(preResult,outPath:string):\n",
    "    f = open(outPath,'w+',encoding = 'utf-8')\n",
    "    for res in preResult:\n",
    "        f.write(\" \".join(res)+\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "def  gradientDescentforPredict(sentences,word_dic,word_count,r1,r2,r3,k2,k3,learning_rate,times):\n",
    "\n",
    "    grad = LinearNGramWIthGrad(sentences,word_dic,word_count,r1,r2,r3,learning_rate=learning_rate)\n",
    "    bestR = [r1,r2,r3]\n",
    "    InPath = \"./data/prep/dev.in\"\n",
    "    OutData = \"./data/prep/dev.out\"\n",
    "    corr = cal_correct(grad,InPath,OutData)\n",
    "    bestT = 0\n",
    "    for i in range(times):\n",
    "        print(\"Turn \",i,)\n",
    "        ppl,descent = grad.cal_ppl(\"./data/lm/train.txt\")\n",
    "        grad.adjustRwithDesc(descent)\n",
    "        \n",
    "        co = cal_correct(grad,InPath,OutData)\n",
    "        if co>corr:\n",
    "            bestT = i\n",
    "            bestR = grad.r[:]\n",
    "            corr = co\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Best Turn %d\\n\\rhyper parameter is \"%(bestT),bestR)\n",
    "    print(\"correct_rate\",corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct rate: 0.7072284780239738\n",
      "Turn  0\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:16.900118\n",
      "descent gradient descent search result: [0.026548848132238224, 0.1315216660282506, -0.15807051416104115]\n",
      "Original R: [0.3, 0.4, 0.3]--->[0.2734511518676107, 0.2684783339716011, 0.4580705141607881]\n",
      "correct rate: 0.7104976389393389\n",
      "Turn  1\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:13.716543\n",
      "descent gradient descent search result: [0.019583908319163765, 0.071345477856925, -0.09092938617597808]\n",
      "Original R: [0.2734511518676107, 0.2684783339716011, 0.4580705141607881]--->[0.25386724354847506, 0.19713285611469794, 0.548999900336827]\n",
      "correct rate: 0.7101343988376316\n",
      "Turn  2\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:12.524823\n",
      "descent gradient descent search result: [0.017756487807274807, 0.052780434024197374, -0.07053692183135526]\n",
      "Original R: [0.25386724354847506, 0.19713285611469794, 0.548999900336827]--->[0.23611075574122783, 0.14435242209051743, 0.6195368221682547]\n",
      "correct rate: 0.7075917181256811\n",
      "Turn  3\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:11.799734\n",
      "descent gradient descent search result: [0.017412767060454224, 0.041479296570945866, -0.05889206363130505]\n",
      "Original R: [0.23611075574122783, 0.14435242209051743, 0.6195368221682547]--->[0.2186979886807944, 0.10287312551958133, 0.6784288857996242]\n",
      "correct rate: 0.6988739556847076\n",
      "Turn  4\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:11.301479\n",
      "descent gradient descent search result: [0.018232050049919433, 0.03261319604840779, -0.05084524609833087]\n",
      "Original R: [0.2186979886807944, 0.10287312551958133, 0.6784288857996242]--->[0.20046593863087425, 0.0702599294711733, 0.7292741318979525]\n",
      "correct rate: 0.6945150744642209\n",
      "Turn  5\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.941629\n",
      "descent gradient descent search result: [0.02044498246970344, 0.024114731868861997, -0.04455971433865822]\n",
      "Original R: [0.20046593863087425, 0.0702599294711733, 0.7292741318979525]--->[0.1800209561611541, 0.046145197602307016, 0.773833846236539]\n",
      "correct rate: 0.6847075917181257\n",
      "Turn  6\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.675763\n",
      "descent gradient descent search result: [0.024486126055133624, 0.014576742108648334, -0.03906286816383471]\n",
      "Original R: [0.1800209561611541, 0.046145197602307016, 0.773833846236539]--->[0.15553483010601227, 0.03156845549365701, 0.8128967144003307]\n",
      "correct rate: 0.6759898292771522\n",
      "Turn  7\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.469138\n",
      "descent gradient descent search result: [0.029546859666370717, 0.00410167885368017, -0.033648538520087805]\n",
      "Original R: [0.15553483010601227, 0.03156845549365701, 0.8128967144003307]--->[0.12598797043963691, 0.02746677663997583, 0.8465452529203873]\n",
      "correct rate: 0.6759898292771522\n",
      "Turn  8\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.286510\n",
      "descent gradient descent search result: [0.030371405370818474, -0.002615110999406769, -0.02775629437145516]\n",
      "Original R: [0.12598797043963691, 0.02746677663997583, 0.8465452529203873]--->[0.09561656506881427, 0.030081887639381293, 0.8743015472918044]\n",
      "correct rate: 0.6908826734471486\n",
      "Turn  9\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.135866\n",
      "descent gradient descent search result: [0.024671169706299245, -0.003743725509340383, -0.02092744419694539]\n",
      "Original R: [0.09561656506881427, 0.030081887639381293, 0.8743015472918044]--->[0.07094539536251598, 0.033825613148722136, 0.8952289914887619]\n",
      "correct rate: 0.6988739556847076\n",
      "Turn  10\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.048113\n",
      "descent gradient descent search result: [0.016506223207413507, -0.0035702520656693882, -0.012935971141780902]\n",
      "Original R: [0.07094539536251598, 0.033825613148722136, 0.8952289914887619]--->[0.054439172155100475, 0.037395865214390145, 0.9081649626305093]\n",
      "correct rate: 0.7072284780239738\n",
      "Turn  11\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.015812\n",
      "descent gradient descent search result: [0.007520976049677363, -0.002921365641340136, -0.004599610408308843]\n",
      "Original R: [0.054439172155100475, 0.037395865214390145, 0.9081649626305093]--->[0.04691819610542445, 0.04031723085573143, 0.9127645730388442]\n",
      "correct rate: 0.70904467853251\n",
      "Turn  12\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.011334\n",
      "descent gradient descent search result: [0.0010967986716056194, -0.0018192865790734583, 0.0007224879074644072]\n",
      "Original R: [0.04691819610542445, 0.04031723085573143, 0.9127645730388442]--->[0.04582139743381867, 0.04213651743480474, 0.9120420851313765]\n",
      "correct rate: 0.7101343988376316\n",
      "Turn  13\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.011109\n",
      "descent gradient descent search result: [-0.0007099145567049833, -0.0007358899666174752, 0.0014458045233212067]\n",
      "Original R: [0.04582139743381867, 0.04213651743480474, 0.9120420851313765]--->[0.046531311990523606, 0.04287240740142217, 0.9105962806080543]\n",
      "correct rate: 0.7101343988376316\n",
      "Turn  14\n",
      "the perplexity of Linear Interpolation on ./data/lm/train.txt is:10.010889\n",
      "descent gradient descent search result: [-0.0005482729260832214, -0.00016730450138510052, 0.0007155774274691634]\n",
      "Original R: [0.046531311990523606, 0.04287240740142217, 0.9105962806080543]--->[0.04707958491660686, 0.0430397119028073, 0.9098807031805858]\n",
      "correct rate: 0.7097711587359244\n",
      "-------------------------------------------------\n",
      "Best Turn 0\n",
      "hyper parameter is  [0.2734511518676107, 0.2684783339716011, 0.4580705141607881]\n",
      "correct_rate 0.7104976389393389\n"
     ]
    }
   ],
   "source": [
    "gradientDescentforPredict(sentences,word_dic,word_count,r1=0.3,r2=0.4,r3=0.3,k2=0.01,k3=0.0001,learning_rate=0.005,times = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct rate: 0.9569701280227596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9569701280227596"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use 0.2734511518676107, 0.2684783339716011, 0.4580705141607881\n",
    "model = LinearNGramWIthGrad(sentences,word_dic,word_count,0.2734511518676107, 0.2684783339716011, 0.4580705141607881)\n",
    "result = predict(model,\"./data/prep/test.in\")\n",
    "saveFile(result,\"./test.out\")\n",
    "cal_correct(model,\"./data/prep/test.in\",\"C:/Users/84200/Documents/WeChat Files/wxid_lt32y9km4l7s21/FileStorage/File/2022-10/test_pred.out\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
